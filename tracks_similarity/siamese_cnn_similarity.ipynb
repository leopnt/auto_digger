{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "import librosa\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # to silence librosa warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Flatten, Dense, Lambda, Dropout, GlobalAveragePooling1D, GlobalMaxPooling1D, Concatenate, LeakyReLU\n",
    "from tensorflow.keras.optimizers.legacy import Adam, RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from triplet_dataset import TripletDataset\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Track:\n",
    "    def __init__(self, filepath: str, sr: int = 22050) -> None:\n",
    "        self.filepath = filepath\n",
    "        self.sr = sr\n",
    "    \n",
    "    def _normalize_mel_spectrogram(mel_spec: np.ndarray) -> np.ndarray:\n",
    "        max_val = np.max(mel_spec)\n",
    "        min_val = np.min(mel_spec)\n",
    "        normalized_spectrogram = (mel_spec - min_val) / (max_val - min_val)\n",
    "\n",
    "        return normalized_spectrogram\n",
    "    \n",
    "    def audio_extract(self, from_sec: int, to_sec: int) -> np.ndarray:\n",
    "        audio, _ = librosa.load(\n",
    "            self.filepath,\n",
    "            mono=True,\n",
    "            sr=self.sr,\n",
    "            offset=from_sec,\n",
    "            duration=to_sec - from_sec\n",
    "        )\n",
    "\n",
    "        if audio is None:\n",
    "            raise Exception(\"Something went wrong went reading extract\")\n",
    "\n",
    "        return audio\n",
    "    \n",
    "    def spectrogram(self, from_sec: int = 40, to_sec: int = 43) -> np.ndarray:\n",
    "        extract = self.audio_extract(from_sec, to_sec)\n",
    "\n",
    "        spec = librosa.feature.melspectrogram(y=extract, sr=self.sr, n_fft=512, hop_length=128)\n",
    "        spec_db = librosa.power_to_db(S=spec, ref=np.max)\n",
    "        spec_db_norm = Track._normalize_mel_spectrogram(spec_db)\n",
    "\n",
    "        return spec_db_norm\n",
    "\n",
    "class TrackPair:\n",
    "    def __init__(self, filepath_left: str, filepath_right: str, similar: bool) -> None:\n",
    "        self.left = Track(filepath_left)\n",
    "        self.right = Track(filepath_right)\n",
    "        self.similar = similar\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, triplets: TripletDataset) -> None:\n",
    "        self.trackpairs: list[TrackPair] = []\n",
    "\n",
    "        for _, row in triplets.df.iterrows():\n",
    "            file_pair = (row[\"anchor\"], row[\"positive\"])\n",
    "            track_pair = TrackPair(file_pair[0], file_pair[1], 1)\n",
    "            self.trackpairs.append(track_pair)\n",
    "\n",
    "            file_pair = (row[\"anchor\"], row[\"negative\"])\n",
    "            track_pair = TrackPair(file_pair[0], file_pair[1], 0)\n",
    "            self.trackpairs.append(track_pair)\n",
    "    \n",
    "    def as_dataframe(self) -> pd.DataFrame:\n",
    "        data = {\"left\": [], \"right\": [], \"similar\": []}\n",
    "\n",
    "        for track_pair in self.trackpairs:\n",
    "            data[\"left\"].append(track_pair.left.filepath)\n",
    "            data[\"right\"].append(track_pair.right.filepath)\n",
    "            data[\"similar\"].append(int(track_pair.similar))\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def as_training_data(self) -> tuple[np.ndarray, np.ndarray]:\n",
    "        pairs = []\n",
    "        labels = []\n",
    "\n",
    "        for track_pair in tqdm(self.trackpairs):\n",
    "            pairs.append([track_pair.left.spectrogram(), track_pair.right.spectrogram()])\n",
    "            labels.append(track_pair.similar)\n",
    "        \n",
    "        return np.array(pairs).astype(float), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = TripletDataset(os.environ[\"PATH_TO_TRACKS\"], n = 2)\n",
    "dataset = Dataset(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_track_spec(track: Track):\n",
    "    plt.figure(figsize=(10, 2))\n",
    "\n",
    "    librosa.display.specshow(track.spectrogram(), y_axis='mel')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.title(os.path.basename(track.filepath))\n",
    "    plt.show()\n",
    "\n",
    "def plot_spec(spec: np.ndarray):\n",
    "    plt.figure(figsize=(10, 2))\n",
    "\n",
    "    librosa.display.specshow(spec, y_axis='mel')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trackpair in dataset.trackpairs[:10]:\n",
    "    print(f\"Similar: {trackpair.similar}\")\n",
    "    plot_track_spec(trackpair.left)\n",
    "    plot_track_spec(trackpair.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dataset.as_training_data()\n",
    "y = np.float32(y)\n",
    "\n",
    "print(f\"X.shape: {X.shape}\")\n",
    "print(f\"y.shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check input data\n",
    "print(f\"Label: {y[0]}\")\n",
    "plot_spec(X[0][0])\n",
    "plot_spec(X[0][1])\n",
    "\n",
    "print(f\"Label: {y[-7]}\")\n",
    "plot_spec(X[-7][0])\n",
    "plot_spec(X[-7][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X.shape[2:]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalL2Pooling1D(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.sqrt(tf.reduce_sum(tf.square(inputs), axis=1))\n",
    "\n",
    "leaky_relu_layer = LeakyReLU(alpha=0.01)\n",
    "\n",
    "def euclidean_distance(embeddings):\n",
    "    x, y = embeddings\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, _ = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def contrastive_loss_with_margin(margin):\n",
    "    def contrastive_loss(y_true, y_pred):\n",
    "        square_pred = K.square(y_pred)\n",
    "        margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "        return (y_true * square_pred + (1 - y_true) * margin_square)\n",
    "    return contrastive_loss\n",
    "\n",
    "# Base network is from:\n",
    "# (1) Recommending music on Spotify with deep learning. Sander Dieleman. https://sander.ai/2014/08/05/spotify-cnns.html (accessed 2024-03-23).\n",
    "def build_base_network(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv1D(filters=256, kernel_size=4, activation=leaky_relu_layer)(inputs)\n",
    "    x = MaxPooling1D(pool_size=4)(x)\n",
    "\n",
    "    x = Conv1D(filters=256, kernel_size=4, activation=leaky_relu_layer)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = Conv1D(filters=512, kernel_size=4, activation=leaky_relu_layer)(x)\n",
    "\n",
    "    # global temporal pooling\n",
    "    mean_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    l2_pool = GlobalL2Pooling1D()(x)\n",
    "\n",
    "    pooled_features = Concatenate()([mean_pool, max_pool, l2_pool])\n",
    "\n",
    "    x = Dense(2048, activation=leaky_relu_layer)(pooled_features)\n",
    "    x = Dense(2048, activation=leaky_relu_layer)(x)\n",
    "\n",
    "    outputs = Dense(40)(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "def build_siamese_network(base_network, input_shape):\n",
    "    input_left = Input(shape=input_shape, name=\"input_left\")\n",
    "    input_right = Input(shape=input_shape, name=\"input_right\")\n",
    "\n",
    "    embeddings_left = base_network(input_left)\n",
    "    embeddings_right = base_network(input_right)\n",
    "\n",
    "    outputs = Lambda(euclidean_distance, name='output_layer',\n",
    "        output_shape=eucl_dist_output_shape)([embeddings_left, embeddings_right])\n",
    "    \n",
    "    siamese_network = Model(inputs=[input_left, input_right], outputs=outputs)\n",
    "\n",
    "    return siamese_network\n",
    "\n",
    "base_network = build_base_network(input_shape)\n",
    "model = build_siamese_network(base_network, input_shape)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.00001), loss=contrastive_loss_with_margin(margin=1.0))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    [X_train[:,0], X_train[:,1]],\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=([X_test[:,0], X_test[:,1]], y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dists = model.predict([X_train[:,0], X_train[:,1]])\n",
    "test_dists = model.predict([X_test[:,0], X_test[:,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.title(\"Predicted distances Train\")\n",
    "sns.histplot(train_dists.ravel())\n",
    "plt.xlim(0.0, 2.0)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title(\"Predicted distances Test\")\n",
    "sns.histplot(test_dists.ravel())\n",
    "plt.xlim(0.0, 2.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "very_similars = [spec for i, spec in enumerate(X_test) if test_dists[i] < 0.2]\n",
    "for left, right in very_similars[:3]:\n",
    "    print('================ Similar =================')\n",
    "    plot_spec(left)\n",
    "    plot_spec(right)\n",
    "\n",
    "very_differents = [spec for i, spec in enumerate(X_test) if test_dists[i] > 1.2]\n",
    "for left, right in very_differents[:3]:\n",
    "    print('================ Different =================')\n",
    "    plot_spec(left)\n",
    "    plot_spec(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "    return np.mean(y_pred == y_true)\n",
    "\n",
    "y_pred_train = (train_dists < np.mean(test_dists)).ravel().astype(int)\n",
    "train_accuracy = compute_accuracy(y_train, y_pred_train)\n",
    "\n",
    "y_pred_val = (test_dists < np.mean(test_dists)).ravel().astype(int)\n",
    "val_accuracy = compute_accuracy(y_test, y_pred_val)\n",
    "\n",
    "print(\"Train Accuracy = {} Val accuracy = {}\".format(train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, 1 - test_dists)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_val)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(left_path: str, right_path: str):\n",
    "    spec_left = Track(left_path).spectrogram()\n",
    "    spec_right = Track(right_path).spectrogram()\n",
    "\n",
    "    pred = model.predict([\n",
    "        spec_left.T.reshape(1, input_shape[0], input_shape[1]),\n",
    "        spec_right.T.reshape(1, input_shape[0], input_shape[1])\n",
    "    ], verbose=False)\n",
    "\n",
    "    return pred.ravel()[0]\n",
    "\n",
    "track_path_a = \"/Users/leopnt/Music/TCOTC/Mostly James - Mostly James (TAN005) - 02 Do You Wanna Go-.aiff\"\n",
    "track_path_b = \"/Users/leopnt/Music/TCOTC/Mostly James - Mostly James (TAN005) - 04 Do You Wanna Go- (SPF Remix).aiff\"\n",
    "track_path_c = \"/Users/leopnt/Music/TCOTC/Dj Falcon & Thomas Bangalter - Together - 01 - Together.aiff\"\n",
    "track_path_d = \"/Users/leopnt/Music/TCOTC/14 - Aneurysm.aiff\"\n",
    "track_path_e = \"/Users/leopnt/Music/TCOTC/8 Bat Dream.aiff\"\n",
    "track_path_f = \"/Users/leopnt/Music/TCOTC/05 - Abdul Raeva - Xpress [209645191].aiff\"\n",
    "track_path_g = \"/Users/leopnt/Music/TCOTC/04. Cool Blue Liquid (Maximus Mix).aiff\"\n",
    "track_path_h = \"/Users/leopnt/Music/TCOTC/04 Boney M. - Rasputin.aiff\"\n",
    "track_path_i = \"/Users/leopnt/Music/TCOTC/03 - Tu deÌgages.aiff\"\n",
    "\n",
    "print(\"a vs e\", distance(track_path_a, track_path_a))\n",
    "print(\"a vs b\", distance(track_path_a, track_path_b))\n",
    "print(\"a vs c\", distance(track_path_a, track_path_c))\n",
    "print(\"a vs d\", distance(track_path_a, track_path_d))\n",
    "print(\"a vs e\", distance(track_path_a, track_path_e))\n",
    "print(\"a vs f\", distance(track_path_a, track_path_f))\n",
    "print(\"a vs g\", distance(track_path_a, track_path_g))\n",
    "print(\"a vs h\", distance(track_path_a, track_path_h))\n",
    "print(\"a vs i\", distance(track_path_a, track_path_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
