{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "import librosa\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # to silence librosa warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Flatten, Dense, Lambda, Dropout, GlobalAveragePooling1D, GlobalMaxPooling1D, Concatenate, LeakyReLU\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from triplet_dataset import TripletDataset\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = TripletDataset(os.environ['PATH_TO_TRACKS'], n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(triplets.df))\n",
    "triplets.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Track:\n",
    "    def __init__(self, filepath: str, sr: int = 22050) -> None:\n",
    "        self.filepath = filepath\n",
    "        self.sr = sr\n",
    "    \n",
    "    def _normalize_mel_spectrogram(mel_spec: np.ndarray) -> np.ndarray:\n",
    "        max_val = np.max(mel_spec)\n",
    "        min_val = np.min(mel_spec)\n",
    "        normalized_spectrogram = (mel_spec - min_val) / (max_val - min_val)\n",
    "\n",
    "        return normalized_spectrogram\n",
    "    \n",
    "    def audio_extract(self, from_sec: int, to_sec: int) -> np.ndarray:\n",
    "        audio, _ = librosa.load(\n",
    "            self.filepath,\n",
    "            mono=True,\n",
    "            sr=self.sr,\n",
    "            offset=from_sec,\n",
    "            duration=to_sec - from_sec\n",
    "        )\n",
    "\n",
    "        if audio is None:\n",
    "            raise Exception(\"Something went wrong went reading extract\")\n",
    "\n",
    "        return audio\n",
    "    \n",
    "    def spectrogram(self, from_sec: int = 30, to_sec: int = 36) -> np.ndarray:\n",
    "        extract = self.audio_extract(from_sec, to_sec)\n",
    "\n",
    "        spec = librosa.feature.melspectrogram(y=extract, sr=self.sr, n_fft=512, hop_length=128)\n",
    "        spec_db = librosa.power_to_db(S=spec, ref=np.max)\n",
    "        spec_db_norm = Track._normalize_mel_spectrogram(spec_db)\n",
    "\n",
    "        return spec_db_norm\n",
    "    \n",
    "    #def trispectrogram(self, offset: float = 1.0) -> np.ndarray:\n",
    "    #    \"\"\"\n",
    "    #    Take 3 spectrogram of n seconds at 25%, 50% and 75% of the track into one\n",
    "\n",
    "    #    Params\n",
    "    #    ======\n",
    "    #    `offset`: offset the start of the spectrograms by `offset` percent.\n",
    "    #    Usefull for data augmentation\n",
    "    #    \"\"\"\n",
    "    #    total_length_sec = len(self.audio) / self.sr\n",
    "    #    n = 5\n",
    "\n",
    "    #    start_25 = int(0.25 * offset * total_length_sec)\n",
    "    #    stop_25 = start_25 + n\n",
    "    #    start_50 = int(0.50 * offset * total_length_sec)\n",
    "    #    stop_50 = start_50 + n\n",
    "    #    start_75 = int(0.75 * offset * total_length_sec)\n",
    "    #    stop_75 = start_75 + n\n",
    "\n",
    "    #    spec_1 = self.spectrogram(start_25, stop_25)\n",
    "    #    spec_2 = self.spectrogram(start_50, stop_50)\n",
    "    #    spec_3 = self.spectrogram(start_75, stop_75)\n",
    "\n",
    "    #    return np.concatenate([spec_1, spec_2, spec_3], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache to spectrograms\n",
    "\n",
    "Because reading audio files is slow. We store the spectograms directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_to_file(data_dict, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        joblib.dump(data_dict, filename)\n",
    "\n",
    "def load_dict_from_file(filename):\n",
    "    data_dict = {}\n",
    "    if os.path.exists(filename):\n",
    "        data_dict = joblib.load(filename)\n",
    "    return data_dict\n",
    "\n",
    "spectrogram_cache: dict = load_dict_from_file('spectrogram_cache.joblib')\n",
    "all_unique_track_paths = pd.unique(triplets.df.values.ravel())\n",
    "\n",
    "if not spectrogram_cache:\n",
    "    spectrogram_cache = {}\n",
    "\n",
    "    for track_path in tqdm(all_unique_track_paths):\n",
    "        track = Track(track_path)\n",
    "\n",
    "        spectrogram_cache[track_path] = track.spectrogram()\n",
    "\n",
    "    #save_dict_to_file(spectrogram_cache, 'spectrogram_cache.joblib')\n",
    "\n",
    "# load new files that are not present in cache\n",
    "count = 0\n",
    "for track_path in tqdm(all_unique_track_paths):\n",
    "    if track_path not in spectrogram_cache.keys():\n",
    "        track = Track(track_path)\n",
    "\n",
    "        spectrogram_cache[track_path] = track.trispectrogram()\n",
    "        count += 1\n",
    "\n",
    "if count > 0:\n",
    "    #save_dict_to_file(spectrogram_cache, 'spectrogram_cache.joblib')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_cache[list(spectrogram_cache.keys())[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spec(track: str, spectrogram_cache: dict):\n",
    "    plt.figure(figsize=(10, 2))\n",
    "\n",
    "    librosa.display.specshow(spectrogram_cache[track], y_axis='mel')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.title(os.path.basename(track))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, triplet in triplets.df[:3].iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(\"Anchor\")\n",
    "    plot_spec(triplet['anchor'], spectrogram_cache)\n",
    "    print(\"Positive\")\n",
    "    plot_spec(triplet['positive'], spectrogram_cache)\n",
    "    print(\"Negative\")\n",
    "    plot_spec(triplet['negative'], spectrogram_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data(triplets, spectrogram_cache):\n",
    "    X_anchor = np.array([spectrogram_cache[path].T for path in triplets.df['anchor']])\n",
    "    X_positive = np.array([spectrogram_cache[path].T for path in triplets.df['positive']])\n",
    "    X_negative = np.array([spectrogram_cache[path].T for path in triplets.df['negative']])\n",
    "\n",
    "    # For triplet loss, y is not directly used during training\n",
    "    # We can return X and an empty y or just X\n",
    "    return X_anchor, X_positive, X_negative\n",
    "\n",
    "X_anchor, X_positive, X_negative = training_data(triplets, spectrogram_cache)\n",
    "\n",
    "print(f\"X_anchor shape: {X_anchor.shape}\")\n",
    "print(f\"X_positive shape: {X_positive.shape}\")\n",
    "print(f\"X_negative shape: {X_negative.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for compatibility with CNN, add one channel\n",
    "X_anchor_gray = X_anchor.reshape(X_anchor.shape[0], X_anchor.shape[1], X_anchor.shape[2])\n",
    "X_positive_gray = X_positive.reshape(X_positive.shape[0], X_positive.shape[1], X_positive.shape[2])\n",
    "X_negative_gray = X_negative.reshape(X_negative.shape[0], X_negative.shape[1], X_negative.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_anchor_gray.shape[1:]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalL2Pooling1D(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.sqrt(tf.reduce_sum(tf.square(inputs), axis=1))\n",
    "\n",
    "leaky_relu_layer = LeakyReLU(alpha=0.3)\n",
    "\n",
    "# Base network is from:\n",
    "# (1) Recommending music on Spotify with deep learning. Sander Dieleman. https://sander.ai/2014/08/05/spotify-cnns.html (accessed 2024-03-23).\n",
    "def build_base_network(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv1D(filters=256, kernel_size=4, activation=leaky_relu_layer)(inputs)\n",
    "    x = MaxPooling1D(pool_size=4)(x)\n",
    "\n",
    "    x = Conv1D(filters=256, kernel_size=4, activation=leaky_relu_layer)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = Conv1D(filters=512, kernel_size=4, activation=leaky_relu_layer)(x)\n",
    "\n",
    "    # global temporal pooling\n",
    "    mean_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    l2_pool = GlobalL2Pooling1D()(x)\n",
    "\n",
    "    pooled_features = Concatenate()([mean_pool, max_pool, l2_pool])\n",
    "\n",
    "    x = Dense(2048, activation=leaky_relu_layer)(pooled_features)\n",
    "    x = Dense(2048, activation=leaky_relu_layer)(x)\n",
    "\n",
    "    outputs = Dense(64)(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "def build_siamese_network(base_network, input_shape):\n",
    "    input_anchor = Input(shape=input_shape, name=\"anchor_input\")\n",
    "    input_positive = Input(shape=input_shape, name=\"positive_input\")\n",
    "    input_negative = Input(shape=input_shape, name=\"negative_input\")\n",
    "\n",
    "    embeddings_anchor = base_network(input_anchor)\n",
    "    embeddings_positive = base_network(input_positive)\n",
    "    embeddings_negative = base_network(input_negative)\n",
    "    \n",
    "    outputs = tf.concat([embeddings_anchor, embeddings_positive, embeddings_negative], axis=1)\n",
    "    \n",
    "    siamese_network = Model(inputs=[input_anchor, input_positive, input_negative], outputs=outputs)\n",
    "\n",
    "    return siamese_network\n",
    "\n",
    "def triplet_loss(y_true, y_pred, margin = 0.2):\n",
    "    anchor, positive, negative = y_pred[:, 0], y_pred[:, 1], y_pred[:, 2]\n",
    "    \n",
    "    pos_dist = K.sum(K.square(anchor - positive), axis=-1)\n",
    "    neg_dist = K.sum(K.square(anchor - negative), axis=-1)\n",
    "    \n",
    "    loss = K.maximum(0.0, pos_dist - neg_dist + margin)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "base_network = build_base_network(input_shape)\n",
    "model = build_siamese_network(base_network, input_shape)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.00001), loss=triplet_loss)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_anchor, X_test_anchor, X_train_positive, X_test_positive, X_train_negative, X_test_negative = train_test_split(\n",
    "    X_anchor_gray, X_positive_gray, X_negative_gray, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    [\n",
    "        X_train_anchor.reshape((-1, input_shape[0], input_shape[1])),\n",
    "        X_train_positive.reshape((-1, input_shape[0], input_shape[1])),\n",
    "        X_train_negative.reshape((-1, input_shape[0], input_shape[1]))\n",
    "    ],\n",
    "    np.zeros_like(X_train_anchor), # dummy labels\n",
    "    epochs=5,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec_dist(left_spec: np.ndarray, right_spec: np.ndarray):\n",
    "    spec_left = left_spec.reshape((1, input_shape[0], input_shape[1]))\n",
    "    spec_right = right_spec.reshape((1, input_shape[0], input_shape[1]))\n",
    "\n",
    "    embeddings_left = base_network.predict(spec_left, verbose=False).ravel()\n",
    "    embeddings_right = base_network.predict(spec_right, verbose=False).ravel()\n",
    "\n",
    "    euclidean_dist = np.linalg.norm(embeddings_left - embeddings_right)\n",
    "    return euclidean_dist\n",
    "\n",
    "def track_dist(left_path: str, right_path: str):\n",
    "    spec_left = spectrogram_cache[left_path].reshape((1, input_shape[0], input_shape[1], input_shape[2]))\n",
    "    spec_right = spectrogram_cache[right_path].reshape((1, input_shape[0], input_shape[1], input_shape[2]))\n",
    "\n",
    "    return spec_dist(spec_left, spec_right)\n",
    "\n",
    "similars_dist = []\n",
    "differents_dist = []\n",
    "for x_a, x_p, x_n in tqdm(zip(X_test_anchor, X_test_positive, X_test_negative), total=X_test_anchor.shape[0]):\n",
    "    similars_dist.append(spec_dist(x_a, x_p))\n",
    "    differents_dist.append(spec_dist(x_a, x_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(similars_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(differents_dist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
